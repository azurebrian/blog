{
  
    
        "post0": {
            "title": "Find Machines Vulnerable to Print Nightmare and Remediate",
            "content": "It‚Äôs been a while, and boy was it a Spring/Summer to remember regarding Microsoft vulnerabilities. Print Nightmare (CVE-2021-34527) scared me enough to want to ensure I went ‚Äúblue team‚Äù enough to prevent any infection. So, I crafted a somewhat crude, but effective, script to detect and remediate on the machines I had influence over. You can find the script in my Github repo. . A couple of things to point out . You‚Äôll notice right at the top that my intention was to disable the print spooler service altogether where I could get away with it: . #For more info: https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-34527 #For machines where printing is not needed and we can just disable printing altogether!!! &lt;# Get-Service -Name Spooler Stop-Service -Name Spooler -Force Set-Service -Name Spooler -StartupType Disabled #&gt; . Of course, on user machines that‚Äôs a little more difficult and hence the trouble to detect and remediate those machines. The most fool-proof way to detect if the appropriate patch has been installed is to search for it: . $Session = New-Object -ComObject &quot;Microsoft.Update.Session&quot; $Searcher = $Session.CreateUpdateSearcher() $historyCount = $Searcher.GetTotalHistoryCount() $hotfix = $Searcher.QueryHistory(0, $historyCount) | Where-Object {$_.Title -like &quot;*KB5004945*&quot;} if ($null -ne $hotfix) { $patchApplied = $True } else { Write-Host &quot;Patch KB5004945 has not been applied and you are at risk. Please install Windows Updates and run this script again.&quot; } . You might be tempted to use the Get-Hotfix Poweshell command to detect for the presence of the installed update. Sometimes that works, but not always according to this post. . The rest of the script is pretty much taken from the Microsoft Security Update Guide for this vulnerability. Please have a look and feel free to drop a comment regarding any similar solutions you might have come up with. And feel free to download, mod, and even open a PR if you like. . Thanks for reading! .",
            "url": "https://azurebrian.com/security/blue%20team/2021/10/18/Remediate-PrintNightmare.html",
            "relUrl": "/security/blue%20team/2021/10/18/Remediate-PrintNightmare.html",
            "date": " ‚Ä¢ Oct 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Folding@home With Your Spare Azure Credits",
            "content": "Introduction | ARM Template | Azure CLI Deployment | Installing and Configuring Folding@home software | Observations So Far | Next Steps | Conclusion | Introduction . If you‚Äôre like me, the whole COVID-19 pandemic has made you think about how each one us can help do our part to get us out of this mess and on to brighter days. Proper social distancing and following the basic guidelines is no small feat to be sure, but what about contributing to actually finding a cure or vaccine. If you‚Äôre not an epidemiologist or Dr. Anthony Fauci, figuring out how you can contribute to those noble causes, might be less obvious. There actually is a way that you can put your IT expertise and resources to good use towards these goals, though. . There is a project called Folding@home that uses a distributed computing model to crunch numbers for finding cures for different diseases in humans. According to their website, ‚Äúfolding‚Äù refers to the way proteins are folded by human cells and this is critical for understanding how the human body reacts to certain diseases. In order for cures to be found, tests and calculations have to be run. The more resources are donated to perform these calculations, the quicker progress is made. So, anyone with CPU/GPU cycles to spare can make a very real difference by just donating their unused processing power. . The goal of this post is to walk throuh the steps I took to get an environment up and running in Azure for contributing to the Folding@home project. I hope it will serve as a relatively easy-to-use guide for doing just that. It‚Äôs assumed you already have access to an Azure subscription and the rights to add new resources. You‚Äôll also need to be familiar with the Azure CLI or some other means of deploying resources via an ARM (Azure Resource Manager) template. I‚Äôll give you the commands needed for use with the CLI. I‚Äôd also like to acknowledge Josh Heffner and his excellent post which was an invaluable help to me as a blueprint. It‚Äôs a great resource for explaining how to use Azure Spot VMs to save costs while participating in Folding@home. At first, this was my goal as well, but though I tried, I was unable to deploy Spot VMs in any region. So, I gave up and just went with traditional VMs. I‚Äôm not sure if this was just due to high-demand for Spot VMs or if it‚Äôs because they weren‚Äôt supported in the configuration I was trying. No worries, if you can get them deployed, you can use them as substitutes for the steps I outline here and you‚Äôll end up saving on VM costs in the long run. If you‚Äôre successful, please leave a comment below! Maybe I didn‚Äôt quite hold my mouth right or something üòú. Let‚Äôs get started! . ARM Template . As mentioned before, we‚Äôre going to be using an ARM template to describe the resources needed and then deploy them. The main resources we‚Äôre going to be deploying are: . A single resource group (FoldingAtHomeRG in the southcentralus region) that contains all the necessary resources including: A single VMSS or VM Scale Set (FoldingAtHomeVMSS) containing: 2 identical Ubuntu Linux 18.04-LTS VM instances, each configured with a single NIC (FoldingAtHomeVMSS_0 and FoldingAtHomeVMSS_1 respectively) and each configured with a public IP for ease of remote administration | A locally-redundant diagnostic storage account used for VM boot diagnostics (foldingathomergdiag166 Please forgive the random naming here‚Ä¶) | . | A single Virtual Network (FoldingAtHomeRG-vnet) with one subnet (10.1.0.0/24) | A single NSG or Network Security Group applied to the subnet that allows for remote access via the Internet (FoldingAtHomeVMSSAccess-nsg) | . | . An ARM template is great for this scenario. It allows us to again, describe, the resources we want to deploy, and then tweak and repeat until we‚Äôre satisfied. In our case, we have a template file and an associated parameters file. You can find these and the CLI commands we‚Äôll use for deploying in my FoldingAtHomeWithAzure repo. The template is essentially the baseline code that indicates the resources and the parameters file lets you, you guessed it, parameterize any specifics. For instance, if you want to deploy 4 instances instead of the 2 I have configured, you can adjust the value for the InstanceCount parameter to your liking: . &quot;instanceCount&quot;: { &quot;value&quot;: &quot;4&quot; }, . This really helps automate things instead of pointing-and-clicking through the portal. I‚Äôve found that it does pay to go through the portal and configure what you want first though. Then you can dowload a template to serve as a starting point. . I won‚Äôt go through the template line-by-line, but I do want to call out some important areas: . The template is currently set to use Standard_NV6 VMs for the VMSS instances: | . &quot;instanceSize&quot;: { &quot;value&quot;: &quot;Standard_NV6&quot; }, . As the link indicates, these use GPUs and, to crunch the most numbers when doing those folding calculations, you need to use something that offloads the processing to a GPU. To go along with the VM instanceSize, you can also specify a priority: . &quot;priority&quot;: { &quot;value&quot;: &quot;Regular&quot; }, . If you wanted to try and use Azure Spot VMs as mentioned a few paragraphs ago, you would set this value to Spot. . There are several places in the parameters file, where you‚Äôll need to substitute &lt;insert your subscription id here&gt; with your actual subscription value. For example, when specifying the virtualNetworkId, you‚Äôll need to use a valid subscription value: | . &quot;virtualNetworkId&quot;: { &quot;value&quot;: &quot;/subscriptions/&lt;insert your subscription id here&gt;/resourceGroups/FoldingAtHomeRG/providers/Microsoft.Network/virtualNetworks/FoldingAtHomeRG-vnet&quot; }, . As mentioned above and per security best practices, an NSG is deployed and associated at the subnet level. This way any devices deployed to that subnet are governed by the same rules. It‚Äôs a much more scalable approach than trying to associate these with each VM‚Äôs NIC, even if we are using an ARM template to automate the deployment. Much as with the subscription id, you‚Äôll need to substitute valid values for your public source IP for the NSG definitions: | . &quot;rules&quot;: [ { &quot;name&quot;: &quot;SSH&quot;, &quot;properties&quot;: { &quot;protocol&quot;: &quot;TCP&quot;, &quot;sourcePortRange&quot;: &quot;*&quot;, &quot;destinationPortRange&quot;: &quot;22&quot;, &quot;sourceAddressPrefix&quot;: &quot;&lt;insert your public source IP here&gt;&quot;, &quot;destinationAddressPrefix&quot;: &quot;*&quot;, &quot;access&quot;: &quot;Allow&quot;, &quot;priority&quot;: 300, &quot;direction&quot;: &quot;Inbound&quot;, &quot;sourcePortRanges&quot;: [], &quot;destinationPortRanges&quot;: [], &quot;sourceAddressPrefixes&quot;: [], &quot;destinationAddressPrefixes&quot;: [] } }, { &quot;name&quot;: &quot;RDP&quot;, &quot;properties&quot;: { &quot;protocol&quot;: &quot;*&quot;, &quot;sourcePortRange&quot;: &quot;*&quot;, &quot;destinationPortRange&quot;: &quot;3389&quot;, &quot;sourceAddressPrefix&quot;: &quot;&lt;insert your public source IP here&gt;&quot;, &quot;destinationAddressPrefix&quot;: &quot;*&quot;, &quot;access&quot;: &quot;Allow&quot;, &quot;priority&quot;: 310, &quot;direction&quot;: &quot;Inbound&quot;, &quot;sourcePortRanges&quot;: [], &quot;destinationPortRanges&quot;: [], &quot;sourceAddressPrefixes&quot;: [], &quot;destinationAddressPrefixes&quot;: [] } } ] . The NSG rules allow SSH and RDP traffic originating from only your public source IP to the subnet where your VM instances reside. I like using this approach when I don‚Äôt want to deploy a VPN Gateway or when it‚Äôs just me accessing my resources. It‚Äôs definitely not the most scalable approach should multiple entities need to manage the resources, but for these pet projects, it does the trick. Without these rules and without configuring an alternative method of access, you won‚Äôt have access to your VM instances. Supposedly, you can use the Azure Serial Console but I was unable to login via this method. It really isn‚Äôt meant as a substitute for other access methods either, but YMMV. . The username of the administration account is set to be foldingadmin: . &quot;adminUsername&quot;: { &quot;value&quot;: &quot;foldingadmin&quot; }, &quot;adminPublicKey&quot;: { &quot;reference&quot;: { &quot;keyVault&quot;: { &quot;id&quot;: &quot;/subscriptions/&lt;insert your subscription id here&gt;/resourceGroups/bhd-key-vault-RG/providers/Microsoft.KeyVault/vaults/bhd-key-vault&quot; }, &quot;secretName&quot;: &quot;ssh-public&quot; } }, . Obviously, you can customize this to be whatever you like. Since we‚Äôre using SSH and we want to keep our credentials out of any template files and source code repos, I have updated the template to reference my public key. I stored this in my Azure KeyVault instance ahead of time, so you‚Äôll want to make sure you do the same before trying to deploy the template. This is awesome, because we can just reference the things we want to keep secure instead of having to copy and paste them around to be fodder for the next hack. . The OS choice is actually set in the template and not in the parameters file. Take a look: | . &quot;virtualMachineProfile&quot;: { &quot;storageProfile&quot;: { &quot;osDisk&quot;: { &quot;createOption&quot;: &quot;fromImage&quot;, &quot;caching&quot;: &quot;ReadWrite&quot;, &quot;managedDisk&quot;: { &quot;storageAccountType&quot;: &quot;[parameters(&#39;osDiskType&#39;)]&quot; } }, &quot;imageReference&quot;: { &quot;publisher&quot;: &quot;Canonical&quot;, &quot;offer&quot;: &quot;UbuntuServer&quot;, &quot;sku&quot;: &quot;18.04-LTS&quot;, &quot;version&quot;: &quot;latest&quot; } }, . You can see here that we‚Äôre creating this from an image and with the 18.04-LTS sku. . The creation and association of a public IP for each instance is handled in the template under the networkProfile section: | . &quot;networkProfile&quot;: { &quot;copy&quot;: [ { &quot;name&quot;: &quot;networkInterfaceConfigurations&quot;, &quot;count&quot;: &quot;[length(parameters(&#39;networkInterfaceConfigurations&#39;))]&quot;, &quot;input&quot;: { &quot;name&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].name]&quot;, &quot;properties&quot;: { &quot;primary&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].primary]&quot;, &quot;enableAcceleratedNetworking&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].enableAcceleratedNetworking]&quot;, &quot;ipConfigurations&quot;: [ { &quot;name&quot;: &quot;[concat(parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].name, &#39;-defaultIpConfiguration&#39;)]&quot;, &quot;properties&quot;: { &quot;subnet&quot;: { &quot;id&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].subnetId]&quot; }, &quot;primary&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].primary]&quot;, &quot;applicationGatewayBackendAddressPools&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].applicationGatewayBackendAddressPools]&quot;, &quot;loadBalancerBackendAddressPools&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].loadBalancerBackendAddressPools]&quot;, &quot;loadBalancerInboundNatPools&quot;: &quot;[parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].loadBalancerInboundNatPools]&quot;, &quot;publicIPAddressConfiguration&quot;: &quot;[if( equals( parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].pipName, &#39;&#39;), json(&#39;null&#39;), union(json(concat(&#39;{ &quot;name &quot;: &quot;&#39;, parameters(&#39;networkInterfaceConfigurations&#39;)[copyIndex(&#39;networkInterfaceConfigurations&#39;)].pipName, &#39; &quot;}&#39;)),json(&#39;{ &quot;properties &quot;: { &quot;idleTimeoutInMinutes &quot;: 15}}&#39;)))]&quot; . If memory serves, the publicIPAddressConfiguration key basically just has to be set to a valid name, which we‚Äôre supplying in the parameters file for pipName: . &quot;networkInterfaceConfigurations&quot;: { &quot;value&quot;: [ { &quot;name&quot;: &quot;FoldingAtHomeRG-vnet-nic01&quot;, &quot;primary&quot;: true, &quot;subnetId&quot;: &quot;/subscriptions/&lt;insert your subscription id here&gt;/resourceGroups/FoldingAtHomeRG/providers/Microsoft.Network/virtualNetworks/FoldingAtHomeRG-vnet/subnets/default&quot;, &quot;applicationGatewayBackendAddressPools&quot;: [], &quot;loadBalancerBackendAddressPools&quot;: [], &quot;applicationSecurityGroups&quot;: [], &quot;loadBalancerInboundNatPools&quot;: [], &quot;enableAcceleratedNetworking&quot;: false, &quot;nsgName&quot;: &quot;&quot;, &quot;nsgId&quot;: &quot;&quot;, &quot;pipName&quot;: &quot;FoldingAtHomeInstance-pip&quot; . Last, but not least for the template, you‚Äôll see where I tried to use the same tag folding@home for all of the associated resources: &quot;tags&quot;: { &quot;application&quot;: &quot;folding@home&quot; } . | . Even though all of these resources are scoped to the same resource group, using tags can be a helpful way to allow for searching and filtering later on. . I think that about covers the important things I wanted to go over via the template. I will say that there‚Äôs a cool VS Code extension that I use to sometimes visualize a template: . . It‚Äôs called ARM Template Viewer, is authored by Ben Coleman, and is averaging 4.9 out of 5 stars on Visual Studio Marketplace currently. It‚Äôs great for when you want to see a graphical representation of your template. You also can import a parameters file, like the one we‚Äôre using here, to allow you to click on a resource and see the specifics of the parameters defined. I highly recommend it and kudos to Ben for his hard work. . Azure CLI Deployment . So, now that we‚Äôve got our template and our parameters file all fixed up, it‚Äôs time to deploy. For this, I like to use Azure CLI commands. You can install it from here. In the same repo, you‚Äôll find the deployVMSS.azcli file containing the commands we‚Äôll need. Of course, the first thing you must do is login: . az login . Now that we‚Äôve gotten that out of the way, let‚Äôs look at the commands: . Find which VM skus are supported in which region: | . az vm list-skus --location eastus2 --size Standard_NV --output table . To find if a particular VM series is available in a specific region, you can use some variant of the above command. This will help avoid errors when you actually deploy. . Update your KeyVault | . az keyvault update --name bhd-key-vault --enabled-for-template-deployment true . In the last section I mentioned how referencing secrets from Azure Key Vault is a great thing. However, apparently, you must explicitly tell your Key Vault instance you want to do this. Now you can add that public SSH key: . az keyvault secret set --vault-name bhd-key-vault --name &quot;ssh-public&quot; --value &lt;insert your public key value&gt; . Validate your deployment | . Ok, here‚Äôs the meat and potatos of all of this. There‚Äôs a command we can run to validate our template; think checking for compile-time errors: . az deployment group validate ` --resource-group FoldingAtHomeRG ` --template-file template.json ` --parameters parameters.json . Notice we specify the RG and then the paths to the template file and associated parameters file respectively. This will help whittle down any major errors so we can get our files in shape before actually attempting to deploy. . Deploy | . Ok, now that all looks good with our files and we‚Äôve weeded out as many problems as we can before deployment, here we go: . az deployment group create ` --name FoldingAtHomeDeployment ` --resource-group FoldingAtHomeRG ` --template-file template.json ` --parameters parameters.json . Simply give a name for the deployment and specify the RG, template, and parameters files. The deployment name is used to reference the deployment. Hopefully, no errors occur but if they do, think of them as run-time errors. You might have to tweak your template/parameters file still if you happen to run into any. If all goes well, you should see the resources appear in your Azure subscription under the FoldingAtHomeRG in just a few minutes. It took maybe 5 minutes or so when I deployed. Pretty slick! . Installing and Configuring Folding@home software . Ok, now that we‚Äôve got some resources to work with, how do we actually participate in the Folding@home project. I‚Äôd like to point out another great blog post that was a huge help in getting this going. Hugh Xie‚Äôs post was very easy to follow and helped streamline the process. In fact, the choice for using Ubuntu-based VMs was based on this post after seeing how a few SSH commands could get this going. In fact, it‚Äôs so good, I‚Äôm not sure I really have a lot to offer other than just follow his guide! Since you‚Äôve now got resources deployed, you can skip to step 2. From there, he‚Äôll walk you thru downloading the software needed to connect to Folding@home. I recommend to use full system resources when prompted, and for me, I just chose to contribute anonymously for now. Maybe I‚Äôll create or join a team in the future ü§∑‚Äç‚ôÇÔ∏è. After installing the software you should be able to monitor your VM instances both on the machine via the tail and watch commands that Hugh gives you as well as via the VMSS page of your subscription in the Azure portal. . Observations So Far . If you‚Äôve made it this far, I hope you now have Azure resources contributing to the Folding@home project! If you do, that should make you feel really good. If you‚Äôre like me, you‚Äôve learned a lot through this process and are contributing, perhaps ever so slightly, towards helping humanity as a whole. So, congratulations! I did want to just give my observations on what I‚Äôve seen so far as well. Really, it‚Äôs only been a few weeks since I started contributing. I‚Äôm a bit bummed because after I deployed and had this cranking, it only took a couple of days for me to reach the credit limit on my Visual Studio Enterprise subscription ($150/month). The rest of the time, my subscription has been sitting idle. It‚Äôs good that those credits have been used (because they weren‚Äôt before), but I feel like I could be doing so much more. I‚Äôm also bummed because I couldn‚Äôt deploy Spot VMs as I‚Äôve mentioned a few times now. That really would have knocked the cost down, but there also would have been the potential for those VMs to get kicked out when the capacity was needed, and that would‚Äôve wasted time as well. If I were just using a normal PAYGO subscription, I think I could have easily racked up a bill that was at least a couple thousand. I guess I could play with using a less powerful NV series VM or maybe one without a GPU, but I kind of think if I have to do that then, what‚Äôs the point? I‚Äôve also thought about creating a ‚ÄúDonate‚Äù link or trying to get some kind of sponsorship or Patreon page. If you have any ideas, please let me know in the comments below. . All in all though, I really like this solution. I love the fact that I can tweak and redeploy in just a few minutes. Using the VMSS allows for flexibility with scaling should I want to do that in the future. It‚Äôs pretty elegant and once it‚Äôs running, it feels pretty ‚Äúset it and forget it‚Äù. Although, if you are on a non-credit based subscription, I would advise creating a budget and then setting budget alerts so you get notified at different thresholds such as 50%, 100% or whenever you‚Äôd like to be notified. That way, ‚Äúforgetting it‚Äù won‚Äôt mean buying the farm (the server farm maybe, but not the actual‚Ä¶umm‚Ä¶farm‚Ä¶nevermind üòè) . Next Steps . The next immediate task on the horizon which I would like to do is get this template added to the Azure QuickStart Templates site. This is a collection of ARM templates for all kinds of workloads and scenarios. Surprisingly, at least to me, there are no templates for Folding@home workloads. So, I would very much like to get this added in order to make it more widely available. From what I‚Äôve read, this requires a pull request to the Quickstart Templates repo and a bit of standardization. If I can get this done, I‚Äôll update this post or maybe even do a new one on the experience. . Conclusion . I know we‚Äôve covered a fair amount of territory and you may be thinking ‚ÄúGosh‚Ä¶there‚Äôs a lot to get my head around in order to do this.‚Äù I would just say, I hope a lot of the heavy lifting has been accounted for via the template. I certainly have much more to learn regarding ARM Templates, but one doesn‚Äôt have to be an expert in order to use them. In light of this, you just need to do three things: . 1. Update the parameters file placeholders with your specifics. 2. Validate and Deploy 3. Install the Folding@home software and verify. . That‚Äôs it! Using the template as-is should be pretty easy to deploy. And honestly, your help is needed! The COVID-19 pandemic is still raging as are these other diseases, and your horsepower would help expedite finding cures. If you have constructive feedback on the template or ways to make it better, please chime in in the comments below. Thanks and Happy Folding! .",
            "url": "https://azurebrian.com/how-to/2020/06/30/Folding-With-Azure.html",
            "relUrl": "/how-to/2020/06/30/Folding-With-Azure.html",
            "date": " ‚Ä¢ Jun 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Stored Access Policies For Azure Files",
            "content": "Lately, at work, I‚Äôve been helping to lead an implementation of an Azure Files share for a client. This is the second client we‚Äôve set up with one, and it‚Äôs a pretty elegant solution that mitigates the need for an on-prem file server. It‚Äôs also available everywhere and doesn‚Äôt require being connected to a VPN, just to name a couple more benefits. In order to provision access, we usually setup end users with either a mapped drive or, more recently, with Azure Storage Explorer. There are numerous advantages to using Storage Explorer, in my opinion, but that‚Äôs not really the scope of this post. However, the ability to use an SAS (Shared Access Signature) URI is one that is relevant. . There are numerous ways to secure access to an Azure Files share. You can provision an Azure AD account and use traditional RBAC roles at the storage account level. That way when a user logs in from Storage Explorer, the portal, or the portal app, the user gets the desired access. If you‚Äôre using a Classic storage account, this might result in granting too much access to an end user, though. You‚Äôre reduced to using roles such as ‚ÄúContributor‚Äù and at the entire storage account level. If your storage account is of the Resource Manager variety you can use new RBAC roles to get more fine-grained control. If configuring a mapped drive, you can use one of the two storage account keys to setup access. It‚Äôs a bit nuclear because it‚Äôs really intended to be used as administrative access, but the only way to map a drive is to use a storage account key per this article, in fact. You can‚Äôt use an SAS URI as you can with Storage Explorer. As I eluded to earlier, this is just one of the big feathers in the cap for using Storage Explorer over mapped drives. You can also create an account SAS (because it‚Äôs defined at the storage account level), or a Service SAS (defined at the Blob, File, Table, or Queue level) as mentioned in this SAS overview. Whew‚Ä¶I‚Äôm already going cross-eyed :dizzy_face:. But what happens if you have a requirement for controlling access for a contractor or a temporary employee? What‚Äôs the best way to handle that? And what if your share resides in a Classic deployment model and you can‚Äôt take advantage of the more fine-grained RBAC roles? We‚Äôre going to talk about a bit of a hybrid approach using the aforementioned Service SAS. . A plain ol‚Äô SAS without any sort of policy is known as an ad-hoc SAS, as defined in this article. It‚Äôs dubbed ‚Äúad-hoc‚Äù because all of the attributes that define the SAS are stored right in the URI. During my research on the best way to balance access to the share with proper security, I found that the ad-hoc Service SAS (Service because I only wanted to provision access to the Files service) is a decent solution. It‚Äôs great for quick hits and temporary access where the access will expire quickly. When using these, the rule of thumb is to not set the expiration date for too long into the future. If a user‚Äôs contract isn‚Äôt renewed for example, you want the SAS to expire pretty quickly so the risk of unauthorized access is at a minimum. That‚Äôs great, but what if you don‚Äôt know when a user‚Äôs contract will be up, or how many times they‚Äôll be renewed, etc. Also, maybe you don‚Äôt want to have to deal with constantly renewing their access every renewal period. Maybe still you‚Äôd rather be able to revoke a user‚Äôs access on-demand rather than wait for a SAS to expire, leaving the share exposed in the meantime. In the ad-hoc SAS scenario and even the mapped drive scenario, a storage account key is used to sign an SAS or setup administrative access with a mapped drive. This is a problem because if we ever need to expire access or the SAS or storage account key get compromised in any way, you have to regenerate or rotate the storage account keys. This could have major implications on business contnuity. Applications relying on those keys would have to get updated, users using SAS URIs or mapped drives signed with that key would have to be configured with new ones and all of it would have to be remedied basically at the same time. So we need some kind of choke point that decouples us from direct access to the storage account keys. Enter the Stored Access Policy. . In my research, I came across Hussein Salman‚Äôs excellent blog post series on SAS and specifically on using them in conjunction with stored access policies. It really helped me weed thru the confusing landscape of secure, temporary access as I eluded to earlier. Hussein‚Äôs post on policies showed how to configure such a policy for Blob containers via the Azure portal. Yet, to my surprise when I went to the Azure Files section of the portal, there was no such UI for configuring a policy. So, I endeavored to find out how to do it. I eventually figured out how to do it using the Azure CLI and from Storage Explorer as well. The latter I didn‚Äôt even realize until I was writing this blog post. It only takes a few commands or steps and at the end you‚Äôll have an Azure Files Service URI based on a stored access policy which you can hand your end user that‚Äôs as locked down as you like. You‚Äôll also be able to revoke that URI at any time, like when your client informs you they haven‚Äôt renewed the end user‚Äôs contract. And perhaps best of all, you‚Äôll insulate yourself from having to regenerate storage account keys in an unplanned fashion. Let‚Äôs look at the CLI approach first: . Azure CLI . These commands can be found in the Azure CLI documentation and basically follow the CRUD commands you‚Äôd expect. Of course, you need to use az login and login with an account that has sufficient permissions to run these commands. The first command we want to run creates the Shared Access Policy: . az storage share policy create --name OneDayAccessFileShare --share-name &lt;some share name&gt; --expiry 2020-05-10T18:08:01Z --permissions rwdl --account-name &lt;some storage account name&gt; . We‚Äôre creating a Stored Access Policy that‚Äôs scoped to a file share. We give it a good, descriptive name: OneDayAccessFileShare which implies this will be just for a day as we‚Äôre testing. We then specify the share name which has to match the actual share name of course. The expiration value, which for our testing purposes is about a day in the future. This can be set to any point in the future, such as some number of months or years down the road. As mentioned already, you want to try and keep these durations as short as possible, but we will see how to revoke these should we need to expire them manually. We then specify the permissions needed. In our test case here we‚Äôre granting all of them: read, write, delete, list. Finally, we specify the storage account name. . az storage share policy list --account-name &lt;some storage account name&gt; --share-name &lt;some share name&gt; . As you can probably guess, this command just lists out the policy to ensure it took, and you get something that looks like this: . &quot;OneDayAccessFileShare&quot;: { &quot;expiry&quot;: &quot;2020-05-10T18:08:01+00:00&quot;, &quot;permission&quot;: &quot;rwdl&quot;, &quot;start&quot;: null }, . Now that we‚Äôve got our policy created, we can generate an SAS: . az storage share generate-sas --name &lt;some share name&gt; --policy-name OneDayAccessFileShare --account-name &lt;some storage account name&gt; . This generates output that looks something like this: . sv=2018-11-09&amp;si=OneDayAccessFileShare&amp;sr=s&amp;sig=Lpj%2Bqz50nM8J7z4AgMXWzHOe6tzDELjg1sIO5q3/fj0%3D . This is basically a query string that we can append to our file share URL. If you don‚Äôt know your file share URL, the easiest way to find it, in my opinion, is to look at the properties for the file share in the portal. Notice the reference to the policy we created assigned to the si variable. . You would now take the full URI and send it to the end user for access in Storage Explorer. You definitely want to take care in doing this, because anyone with this SAS URI will be granted access. . But what about revoking the signature as we mentioned earlier? There‚Äôs one more command we need to use in order to revoke the SAS if we need to expire it on-demand for any reason: . az storage share policy update --name OneDayAccessFileShare --share-name &lt;some share name --expiry 2020-05-08T18:08:01Z --permissions rwdl --account-name &lt;some share name&gt; . To expire the SAS we update the policy to use an expiration date in the past. This automatically revokes the SAS by way of the policy. Because the SAS gets sent with each request to the share, this will pretty much immediately revoke access to anyone using this SAS URI. You can also just delete the policy, but the good thing about just updating the expiration date is that updating it back to a future date enables the SAS again. So now you have full control over a user‚Äôs access with just one line in the Azure CLI! . Azure Storage Explorer . As I mentioned, while I was writing this post I was pleasantly surprised to find that the UI I had been looking for in the portal is actually baked into Storage Explorer. That‚Äôs ok, it made me get familiar with the relevant Azure CLI commands and it‚Äôs nice to know more than one way to skin a cat. Basically, we‚Äôre going to do the same thing we did with the commands above, just in Storage Explorer. And lest I not mention it, kudos to Microsoft for making Storage Explorer, and the CLI for that matter, cross-platform! This is yet another benefit of Storage Explorer in that it‚Äôs the same access, interface and functionality on any OS. I‚Äôm doing all of this on my MacBook Pro and it‚Äôs the same experience as on Windows. And for those of us supporting different end user preferences and environments, consistency is a great thing. :+1: . Just as with the CLI, you need to be logged in to Storage Explorer with an account that has the appopriate privileges to perform these operations. Let‚Äôs start with creating the policy. Right-click on your file share and you‚Äôll see options similar to these: . . Choose Manage Access Policies and you‚Äôll see a screen similar to this: . . If you‚Äôve already perfomed the CLI steps above, you‚Äôll see one or more policies listed. You‚Äôll notice you basically have the same options available to you as with the CLI steps. One notable exception is the presence of a Create permission. Per the CLI documentation for az storage share policy create the only permissions you can specify via the CLI are rwdl. There is no c or mention of a create permission. Yet, in the portal when creating an ad-hoc Files Service SAS you can specify one and in Storage Explorer you can as well. I don‚Äôt see any difference in behavior in Storage Explorer using either method however, as you can still upload files to the share, create folders and manipulate them as you would like. It is curious to me that there‚Äôs a discrepancy, but I honestly don‚Äôt know why this is. If you know, please comment and share your wisdom! . Once the policy is created, you can right-click again on the share and choose Get Shared Access Signature: . . You‚Äôll see a screen like this: . . At the very top, you can specify the policy you just created and the relevant values will populate based on how you set your policy, other than the Start time: field. It will basically be null. Clicking create will create a URI that you can share with your end user. You don‚Äôt even have to append the SAS with the endpoint! Although, it gives you a query string representation as well :grin:. . Summary . Using Stored Access Policies to control access to resources like Azure Files shares has three distinct advantages: . We create a choke-point where you can control access, particularly to users outside of an organization. | We eliminate the need to rotate/regenerate storage account keys which could impact availability should you need to revoke access. | We allow for fine-grained control of permissions to Azure Files shares no matter which deployment model we‚Äôre using. | I‚Äôd love to hear how you‚Äôre securing your Azure Files shares or any constructive feedback you might have. Hopefully this post will help a little the next time you or I are weeding through all the possible options Azure has to offer. .",
            "url": "https://azurebrian.com/how-to/2020/05/11/Stored-Access-Policies-For-Azure-Files.html",
            "relUrl": "/how-to/2020/05/11/Stored-Access-Policies-For-Azure-Files.html",
            "date": " ‚Ä¢ May 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Azure Fundamentals Certification Preparation",
            "content": "UPDATE: Today, 09-24-2020, I took and passed the AZ-900 exam. I wanted to revisit this post with my thoughts and impressions now that I can look back on the process. Of course, I can‚Äôt dive into specifics regarding the exam or the questions, but suffice it to say that I thought it was very fair. I have to say that the MeasureUp Practice tests prepared me very well. In fact, I think they were harder than the actual exam. So, I highly recommend using them. I also was struck by how much I fell back on what I had experienced by actually using Azure to deploy resources or just play around. So, I would definitely also say that any time you can spend in Azure deploying VMs, configuring resources, or just doing Microsoft Learn modules that allow you to use a sandbox, is well worth it. Furthermore, if you get Azure credits via a Visual Studio subscription through your company or otherwise, take advantage of them. It‚Äôs $150 in free money every month to play around and tinker! I also really appreciated being able to use Tim Warner‚Äôs study guide on specific topics I felt I needed reinforcement on. And Mike Pfeiffer‚Äôs Cloudskills.io podcasts are really good as well. In fact, these fellas have a one-day bootcamp coming up in October. So, if you‚Äôre thinking about starting your cloud journey by taking this exam, it‚Äôs a great time to do it. Good luck and please leave a comment below if you have any other helpful tips or resources! . Greetings! I wanted to do a blog post about studying for the Azure Fundamentals (AZ-900) exam. I‚Äôve technically been using Azure in my consulting position for a little over two years now. However, I‚Äôve really started leaning into it, and getting serious about learning it, within the last year. My goal is to blend my IT administration background with my development career and pursuing Azure, and the solutions it provides, well that really seems like the ideal amalgamation of the two to me. I‚Äôm passionate about doing IT right, and I‚Äôm also passionate about clean development practices. So, I feel this is a pretty exciting time to be learning Azure. I‚Äôve been fortunate to get my hands dirty with real-world projects during this time. However, like many of you I‚Äôm sure, I do want to formalize that experience by obtaining Azure-based certifications. Whether it‚Äôs angling for more money or just wanting others to take you seriously, there‚Äôs no doubt these certications have the potential to unlock doors and change mindsets. . Given all of that, it seemed only natural to start at the ground floor, so to speak. The Azure Fundamentals exam is aimed at a much broader audience than just us techies. However, I feel it may be deceptively hard. No, I don‚Äôt think I‚Äôll have to configure a hybrid network connection or some crazy storage solution, but I also can‚Äôt remember another exam that had the potential to cover such a breadth of material. We all know that Azure is a beast and constanly changing with new offerings and updates to existing ones. So, it‚Äôs a little unnerving when I look at the range of material. I do think it helps to have real-world projects to practice recommending services and pricing estimates, though. So, I hope that‚Äôs building up repetition and the muscle memory I need to retain the knowledge. . The other objective I had for this blog post was to let it serve as a way to curate a list of training resources. I think it might be useful to break it down in terms of curriculum offerings, study guides, practice tests, and podcasts. I soak up a lot of knowledge from podcasts so I want to make sure I include those too. So, here‚Äôs what I‚Äôve got so far: . Curriculum . Azure fundamentals Learning Path - This has been my primary curriculum for studying. It‚Äôs 12 modules. It‚Äôs been great because it allows for some hands-on practice in a free Azure lab environment to cement some concepts. What‚Äôs more is it‚Äôs free and virtual, which is awesome because my company doesn‚Äôt typically pay for training classes. And, it can be done remote, of course, in these crazy times. It really didn‚Äôt take that long to get through and it‚Äôs gamified so you get the thrill of leveling up! | Jim Cheshire‚Äôs Exam Ref AZ-900 Microsoft Azure Fundamentals 1st Edition - I have a sample of this on my Kindle app, but I confess, I haven‚Äôt really looked at it much yet. | . Study Guides . Tim Warner‚Äôs Exam AZ-900 MS Azure Fundamentals Study Guide YouTube Playlist - The awesome Tim Warner has a video series that, when finished, will consist of no less than 63 study guide videos. Free! | Thomas Maurer‚Äôs Study Guide - Likewise, the fantastic Thomas Maurer has just released, like as of 3/30, his offering. Free! | . Practice Tests . Official MeasureUp Practice Test. - You can also find this on the Microsoft AZ-900 Exam Page. I wanted something I could iterate with as a measuring stick regarding the exam objectives. That way, I could see where I would need to focus my studying. There‚Äôs also a test pass gurantee for what it‚Äôs worth. | Chris Pietschmann and Dan Patrick‚Äôs Self Assessment tool - This really isn‚Äôt a practice test so much as it is a tool for keeping track of your progress, but because it makes you continually reassess yourself, this seems like the natural place for it. Free! | . Podcasts . CloudSkills.io Podcast Episode 39 - This podcast features excellent host Mike Pfeiffer talking with Tim Warner about how to prepare for this very exam. | CloudSkills.io Podcast Episode 48 - Another podcast where Mike and Tim discuss this exam. This time even discussing Tim Warner‚Äôs Microsoft Ignite presentation on prep for it. | . So, there‚Äôs a lot of momentum around this exam, and it seems like some new hotness for prepping for it is being released almost daily. I‚Äôm curious as to what others are using to prepare. Do you use a resource not listed here? Leave a comment below and let me know what it is please! I‚Äôll get it added. Also, I‚Äôd be curious to know your approach and if I‚Äôm just being paranoid about the difficulty as it relates to the breadth of the content that could be covered. I‚Äôd love to hear your feedback! Thanks and good luck in your studies! .",
            "url": "https://azurebrian.com/training/2020/03/31/AZ900-Prep.html",
            "relUrl": "/training/2020/03/31/AZ900-Prep.html",
            "date": " ‚Ä¢ Mar 31, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Launch!!!",
            "content": "Greetings! I wanted to write a few lines to capture the milestone represented by this post ‚Äì it‚Äôs the first one! I‚Äôm excited about what this means in terms of my journey in this space, but I do hope it will be a resource for others who are looking for good, helpful Azure-related content as well. Just a quick shout out to some of the folks that have inspired me to launch this blog: . Tim Warner | Chris Pietschmann | Thomas Maurer | John Savill | Troy Hunt | . I respect each of the above‚Äôs expertise first and foremost, and have garnered so much knowledge from their blog posts, Pluralsight courses, newsletters, podcasts, and Twitter accounts. I also admire greatly their professionalism in the way they brand and market themselves. That‚Äôs the ‚Äúblueprint‚Äù, pardon the pun :smirk:, I‚Äôm shooting for here. I hope that as I learn I can give back to the community, as they have. . So, please stay tuned for upcoming posts on topics such as: . AZ-900 Azure Fundamentals Certification | Azure Network Troubleshooting Tools | Hybrid connections with VNAs (Virtual Network Appliances) | . If you want to be informed of these posts as they‚Äôre published, be sure to follow me on Twitter and link to the Atom feed (buttons for both are in the sidebar to the right). Also, if there are any topics you‚Äôd like to see covered, feel free to leave a comment below (Github account required). . Thanks and I look forward to sharing more soon! . Brian .",
            "url": "https://azurebrian.com/general/2020/03/26/launch.html",
            "relUrl": "/general/2020/03/26/launch.html",
            "date": " ‚Ä¢ Mar 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ".",
          "url": "https://azurebrian.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://azurebrian.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}